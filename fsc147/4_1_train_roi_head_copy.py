"""
æ­¤è„šæœ¬ç”¨äºè®­ç»ƒROI Headæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºç›®æ ‡æ£€æµ‹å’Œè®¡æ•°ä»»åŠ¡ã€‚
è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ç‰¹å¾å’Œè¯­è¨€æç¤ºï¼Œä½¿ç”¨SAMï¼ˆSegment Anything Modelï¼‰å’ŒCLIPç‰¹å¾è¿›è¡Œå°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬æ£€æµ‹ä»»åŠ¡ï¼Œ
ä¸»è¦åº”ç”¨äºFSC147æ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹ä¸è®¡æ•°ã€‚

è¾“å…¥è·¯å¾„å’Œæ–‡ä»¶ï¼š
- è¾“å…¥å›¾åƒï¼š'/mnt/mydisk/wjj/dataset/FSC_147/images_384_VarV2/' ç›®å½•ä¸‹çš„å›¾åƒæ–‡ä»¶
- COCOæ ‡æ³¨æ–‡ä»¶ï¼š'/mnt/mydisk/wjj/dataset/FSC_147/annotation_FSC147_384_with_gt.json'
- CLIPæ–‡æœ¬æç¤ºï¼š'{project_root}/data/fsc147/clip_text_prompt.pth'
- è®­ç»ƒæ•°æ®ï¼š'{project_root}/data/fsc147/sam/all_data_vith.pth'
- é¢„æµ‹æ•°æ®ï¼š'{project_root}/data/fsc147/sam/all_predictions_vith.pth'
- ä¼ªæ¡†æ•°æ®ï¼š'{project_root}/data/fsc147/sam/pseudo_boxes_data_vith.pth'

è¾“å‡ºè·¯å¾„å’Œæ–‡ä»¶ï¼š
- æ¨¡å‹æƒé‡ï¼š'{project_root}/data/fsc147/checkpoints/cls_head/ckpt/{run_name}'
- COCOæ ¼å¼çš„è¯„ä¼°ç»“æœï¼šé€šè¿‡COCO APIç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡
- è®¡æ•°è¯„ä¼°æŒ‡æ ‡ï¼šMAE, RMSE, NAE, SREç­‰æŒ‡æ ‡
"""
import os
import sys
import time
import logging

# ====================== æ–°å¢ï¼šå…¨å±€æ—¥å¿—é…ç½® ======================
# åˆ›å»ºæ—¥å¿—å™¨
log_format = '[%(asctime)s] [%(levelname)s] %(message)s'
logging.basicConfig(
    level=logging.INFO,
    format=log_format,
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.StreamHandler(),
        # è¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´å‘½åï¼Œé¿å…è¦†ç›–ï¼‰
        # logging.FileHandler(
        #     f'/mnt/mydisk/wjj/PseCo-main/data/fsc147/logs/train_roi_head_{time.strftime("%Y%m%d_%H%M%S")}.log',
        #     encoding='utf-8'
        # )
    ]
)
logger = logging.getLogger('ROIHead_Trainer')
# ===============================================================

project_root = '/mnt/mydisk/wjj/PseCo-main'
sys.path.insert(0, project_root)
from torchvision.utils import make_grid
from torchvision.transforms.functional import to_pil_image, to_tensor
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch
from PIL import Image
import numpy as np
import tqdm
import albumentations as A
import torch.nn as nn
import torchvision
import torchvision.ops as vision_ops
from ops.ops import _nms, plot_results, convert_to_cuda

# å‘½ä»¤è¡Œå‚æ•°å®šä¹‰
import argparse

parser = argparse.ArgumentParser('Default arguments for training of different methods')
parser.add_argument('--wandb', help='wandb', action='store_true')
parser.add_argument('--zeroshot', help='zeroshot', action='store_true')
parser.add_argument('--arch', help='arch: vitb, vitl, vith', type=str, default='vith')
parser.add_argument('--entity', help='wandb user name', type=str, default='zzhuang')
opts = parser.parse_args()
logger.info(f"âœ… å‘½ä»¤è¡Œå‚æ•°åŠ è½½å®Œæˆ: {opts}")  # æ–°å¢ï¼šæ‰“å°å‚æ•°

# æ•°æ®é›†æ³¨å†Œå’Œåˆå§‹åŒ–================================================
import json
from pycocotools.coco import COCO

# åŠ è½½COCOæ ‡æ³¨æ–‡ä»¶
logger.info("ğŸ“Œ å¼€å§‹åŠ è½½COCOæ ‡æ³¨æ–‡ä»¶...")
try:
    coco_gt = COCO(f"/mnt/mydisk/wjj/dataset/FSC_147/instances_test_val_bin.json")
    img_num = len(coco_gt.dataset.get('images', []))
    ann_num = len(coco_gt.dataset.get('annotations', []))
    logger.info(f"âœ… COCOæ ‡æ³¨æ–‡ä»¶åŠ è½½æˆåŠŸ | å›¾åƒæ•°: {img_num} | æ ‡æ³¨æ•°: {ann_num}")
except Exception as e:
    logger.error(f"âŒ COCOæ ‡æ³¨æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
    raise
# ============================================================
torch.autograd.set_grad_enabled(False)

from ops.foundation_models.segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor, \
    build_sam, build_sam_vit_b, build_sam_vit_h

# æ¨¡å‹åŠ è½½
logger.info("ğŸ“Œ å¼€å§‹åŠ è½½SAMæ¨¡å‹å’Œç‰¹å¾æ•°æ®...")
try:
    if opts.arch == 'vith':
        logger.info("ğŸ”§ åŠ è½½SAM ViT-Hæ¨¡å‹æƒé‡...")
        sam = build_sam_vit_h("/mnt/mydisk/wjj/Prompt_sam_localization/checkpoint/sam_vit_h_4b8939.pth").cuda().eval()
        logger.info("ğŸ”§ åŠ è½½all_data_vith_v5_fix.pthç‰¹å¾æ•°æ®...")
        all_data = torch.load(f'{project_root}/data/fsc147/sam/all_data_vith_v5_fix.pth', map_location='cpu')
        logger.info("ğŸ”§ åŠ è½½all_predictions_vith.pthé¢„æµ‹æ•°æ®...")
        all_predictions = torch.load(f'{project_root}/data/fsc147/sam/all_predictions_vith.pth', map_location='cpu')
        logger.info(f"âœ… SAMæ¨¡å‹å’Œç‰¹å¾æ•°æ®åŠ è½½å®Œæˆ | æ€»æ ·æœ¬æ•°: {len(all_data)}")
    else:
        raise NotImplementedError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹æ¶æ„: {opts.arch}")
except Exception as e:
    logger.error(f"âŒ SAMæ¨¡å‹/ç‰¹å¾æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    raise

# åŠ è½½CLIPæ–‡æœ¬æç¤ºå’Œä¼ªæ ‡ç­¾
logger.info("ğŸ“Œ å¼€å§‹åŠ è½½CLIPæ–‡æœ¬æç¤ºå’Œä¼ªæ ‡ç­¾æ•°æ®...")
try:
    clip_text_prompts = torch.load(f'{project_root}/data/fsc147/clip_text_prompt.pth', map_location='cpu')
    all_pseudo_boxes = torch.load(f'{project_root}/data/fsc147/sam/pseudo_boxes_data_vith.pth', map_location='cpu')
    logger.info("âœ… CLIPæ–‡æœ¬æç¤ºå’Œä¼ªæ ‡ç­¾æ•°æ®åŠ è½½å®Œæˆ")

#     # é¢„å¤„ç†æ•°æ®
#     logger.info("ğŸ“Œ å¼€å§‹é¢„å¤„ç†æ•°æ®ï¼ˆæ·»åŠ image_id/predictions/ä¼ªæ¡†ï¼‰...")
#     for fname in tqdm.tqdm(all_data, desc="é¢„å¤„ç†æ•°æ®"):
#         target = all_data[fname]
#         target['image_id'] = fname
#         target['predictions'] = all_predictions[fname]
#         if all_data[fname]['split'] == 'train':
#             target['annotations']['boxes'] = all_pseudo_boxes[fname]['pred_boxes']
#             target['annotations']['ious'] = all_pseudo_boxes[fname]['pred_ious']
#     logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
except Exception as e:
    logger.error(f"âŒ CLIP/ä¼ªæ ‡ç­¾æ•°æ®åŠ è½½/é¢„å¤„ç†å¤±è´¥: {str(e)}")
    raise

# æ•°æ®é›†åˆ’åˆ†
logger.info("ğŸ“Œ å¼€å§‹åˆ’åˆ†æ•°æ®é›†...")
all_image_list = {'train': [], 'val': [], 'test': [], 'all': []}
for fname in all_data:
    all_image_list[all_data[fname]['split']].append(fname)
    all_image_list['all'].append(fname)
logger.info(
    f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ | train: {len(all_image_list['train'])} | val: {len(all_image_list['val'])} | test: {len(all_image_list['test'])} | all: {len(all_image_list['all'])}"
)

from models import ROIHeadMLP as ROIHead

num_masks = 5
run_name = 'MLP_small_box_w1'
if opts.zeroshot:
    run_name += '_zeroshot'
cls_loss2_weight = 1.0
logger.info(f"ğŸ“Œ å®éªŒé…ç½® | run_name: {run_name} | num_masks: {num_masks} | cls_loss2_weight: {cls_loss2_weight}")

# è®­ç»ƒå™¨åˆå§‹åŒ–
logger.info("ğŸ“Œ åˆå§‹åŒ–è®­ç»ƒå™¨å’Œæ¨¡å‹...")
from ops.loggerx import LoggerX

try:
    loggerx = LoggerX(
        save_root=f'{project_root}/data/fsc147/checkpoints/cls_head/ckpt/{run_name}',
        name=run_name,
        enable_wandb=opts.wandb,
        config=opts,
        entity=opts.entity,
        project='Counting'
    )
    cls_head = ROIHead().cuda()
    loggerx.modules = [cls_head, ]
    optimizer = torch.optim.AdamW(list(cls_head.parameters()), lr=0.0001, weight_decay=0.00001)
    acc_grd_step = 1
    max_iter = 10000
    bs = 64
    logger.info(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ | æœ€å¤§è¿­ä»£æ•°: {max_iter} | æ‰¹æ¬¡å¤§å°: {bs} | å­¦ä¹ ç‡: 1e-4")
except Exception as e:
    logger.error(f"âŒ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    raise

# æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
logger.info("ğŸ“Œ åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒé…ç½®...")
from ops.grad_scaler import NativeScalerWithGradNormCount

amp = True
scaler = NativeScalerWithGradNormCount(amp=amp)
logger.info(f"âœ… æ··åˆç²¾åº¦è®­ç»ƒé…ç½®å®Œæˆ | amp: {amp}")


def evaluate(split, results, threshold=None):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½

    Args:
        split (str): æ•°æ®é›†åˆ’åˆ†ï¼Œå¦‚ 'train', 'val', 'test'
        results (dict): å­˜å‚¨è¯„ä¼°ç»“æœçš„å­—å…¸
        threshold (float, optional): ç”¨äºè®¡æ•°è¯„ä¼°çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸ºNone

    Returns:
        dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…æ‹¬bboxæŒ‡æ ‡å’Œè®¡æ•°æŒ‡æ ‡(MAE, RMSE, NAE, SRE)
    """
    image_list = [fname for fname in all_data if all_data[fname]['split'] == split]
    # =====================================================================
    # from detectron2.evaluation import COCOEvaluator
    #
    # coco_evaluator = COCOEvaluator(dataset_name=f'fsc_test_val', tasks=['bbox', ],
    #                                output_dir=f'{project_root}/data/temp', max_dets_per_image=1000)
    # coco_evaluator.reset()
    #
    # from detectron2.structures import Boxes, ImageList, Instances, RotatedBoxes
    #
    # all_predictions = {}
    # æ›¿æ¢detectron2çš„COCOEvaluatorï¼šä½¿ç”¨åŸç”Ÿpycocotoolsè¯„ä¼°
    from pycocotools.cocoeval import COCOeval

    # å‡†å¤‡COCOè¯„ä¼°çš„é¢„æµ‹ç»“æœåˆ—è¡¨
    coco_preds = []
    all_predictions = {}
    # ==============================================

    for fname in tqdm.tqdm(image_list):
        features = all_data[fname]['features'].cuda()
        with torch.no_grad():
            cls_head.eval()
            if opts.zeroshot:
                example_features = clip_text_prompts[all_data[fname]['class_name']].unsqueeze(0).cuda()
            else:
                example_features = all_data[fname]['example_clip_features'].cuda()

        min_scores = 0.05
        max_points = 1000
        pred_points_score = all_data[fname]['predictions']['pred_points_score']
        mask = torch.zeros(pred_points_score.size(0))
        mask[:min(pred_points_score.size(0), max_points)] = 1
        mask[pred_points_score < min_scores] = 0
        pred_boxes = all_data[fname]['predictions']['pred_boxes'][:, :num_masks][mask.bool()].cuda()
        pred_ious = all_data[fname]['predictions']['pred_ious'][:, :num_masks][mask.bool()].cuda()

        all_pred_boxes = []
        all_pred_scores = []
        for indices in torch.arange(len(pred_boxes)).split(128):
            with torch.no_grad():
                cls_outs_ = cls_head(features, [pred_boxes[indices], ], [example_features, ] * len(indices))
                pred_logits = cls_outs_.sigmoid().view(-1, len(example_features), num_masks).mean(1)
                pred_logits = pred_logits * pred_ious[indices]

                all_pred_boxes.append(pred_boxes[indices, torch.argmax(pred_logits, dim=1)])
                all_pred_scores.append(pred_logits.max(dim=1).values)

        height, width = all_data[fname]['height'], all_data[fname]['width']
        scale = max(height, width) / 1024.
        pred_boxes = torch.cat(all_pred_boxes) * scale
        pred_boxes[:, [0, 2]] = pred_boxes[:, [0, 2]].clamp(0, width)
        pred_boxes[:, [1, 3]] = pred_boxes[:, [1, 3]].clamp(0, height)
        pred_scores = torch.cat(all_pred_scores)
        box_area = vision_ops.box_area(pred_boxes)
        mask = (box_area < (height * width * 0.75)) & (box_area > 10)
        pred_boxes = pred_boxes[mask]
        pred_scores = pred_scores[mask]
        # ======================================
        # nms_indices = vision_ops.nms(pred_boxes, pred_scores, 0.5)
        # instances = Instances((height, width))
        # pred_boxes = pred_boxes[nms_indices]
        # pred_scores = pred_scores[nms_indices]
        # instances.pred_boxes = Boxes(pred_boxes)
        # instances.scores = pred_scores
        # instances.pred_classes = torch.zeros(len(pred_boxes)).cuda().long()
        # prediction = {"image_id": int(fname[:-4]), "instances": instances}
        # coco_evaluator.process(
        #     [{'file_name': fname, 'height': height, 'width': width, 'image_id': int(fname[:-4])}],
        #     [prediction, ])
        # all_predictions[fname] = prediction
        # æ›¿æ¢detectron2çš„Instances/Boxesï¼šåŸç”Ÿå¼ é‡+COCOæ ¼å¼è½¬æ¢
        nms_indices = vision_ops.nms(pred_boxes, pred_scores, 0.5)
        pred_boxes = pred_boxes[nms_indices]
        pred_scores = pred_scores[nms_indices]

        # è½¬æ¢ä¸ºCOCOè¯„ä¼°æ ¼å¼ï¼ˆx1,y1,w,hï¼‰ï¼šåŸä»£ç æ˜¯x1,y1,x2,y2ï¼Œéœ€è¦è½¬æ¢
        image_id = int(fname[:-4])
        for box, score in zip(pred_boxes.cpu().numpy(), pred_scores.cpu().numpy()):
            x1, y1, x2, y2 = box
            w = x2 - x1
            h = y2 - y1
            coco_preds.append({
                "image_id": image_id,
                "category_id": 0,  # ç±»åˆ«IDå’ŒåŸä»£ç ä¿æŒä¸€è‡´ï¼ˆ0ç±»ï¼‰
                "bbox": [float(x1), float(y1), float(w), float(h)],
                "score": float(score)
            })

        # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆç”¨äºè®¡æ•°è¯„ä¼°ï¼‰
        all_predictions[fname] = {
            "image_id": image_id,
            "boxes": pred_boxes,
            "scores": pred_scores
        }

        # break
    # detection_results = coco_evaluator.evaluate([int(x[:-4]) for x in image_list])
    # for k in detection_results['bbox']:
    #     results[split + '_' + k] = detection_results['bbox'][k]
    # è¿è¡ŒåŸç”ŸCOCOè¯„ä¼°ï¼ˆæ›¿ä»£detectron2çš„coco_evaluator.evaluateï¼‰
    coco_dt = coco_gt.loadRes(coco_preds)
    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
    # ç­›é€‰éœ€è¦è¯„ä¼°çš„å›¾ç‰‡ID
    eval_img_ids = [int(x[:-4]) for x in image_list]
    coco_eval.params.imgIds = eval_img_ids
    # æ‰§è¡Œè¯„ä¼°
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

    # æå–è¯„ä¼°æŒ‡æ ‡ï¼ˆå’Œdetectron2è¾“å‡ºæ ¼å¼å¯¹é½ï¼‰
    detection_results = {
        'AP': coco_eval.stats[0],
        'AP50': coco_eval.stats[1],
        'AP75': coco_eval.stats[2],
        'APs': coco_eval.stats[3],
        'APm': coco_eval.stats[4],
        'APl': coco_eval.stats[5],
        'AR1': coco_eval.stats[6],
        'AR10': coco_eval.stats[7],
        'AR100': coco_eval.stats[8],
        'ARs': coco_eval.stats[9],
        'ARm': coco_eval.stats[10],
        'ARl': coco_eval.stats[11]
    }
    for k in detection_results:
        results[split + '_' + k] = detection_results[k]

    def eval_counting(thresh):
        """
        è®¡ç®—è®¡æ•°è¯„ä¼°æŒ‡æ ‡

        Args:
            thresh (float): ç”¨äºåˆ¤æ–­ç›®æ ‡çš„é˜ˆå€¼

        Returns:
            tuple: (mae, mse, nae, sre) - å¹³å‡ç»å¯¹è¯¯å·®ã€å‡æ–¹æ ¹è¯¯å·®ã€å½’ä¸€åŒ–å¹³å‡è¯¯å·®ã€å¹³æ–¹ç›¸å¯¹è¯¯å·®
        """
        total_mae = 0.
        total_mse = 0.
        total_nae = 0.
        total_sre = 0.
        for i, fname in enumerate(image_list):
            num_points = len(all_data[fname]['annotations']['points'])
            # err = abs(num_points - (all_predictions[fname]['instances'].scores > thresh).sum())
            # æ›¿æ¢detectron2çš„instances.scoresï¼šç›´æ¥è®¿é—®ä¿å­˜çš„scoreså¼ é‡
            pred_cnt = (all_predictions[fname]['scores'] > thresh).sum().item()
            err = abs(num_points - pred_cnt)
            total_mae += err
            total_mse += err ** 2
            total_nae += err / num_points
            total_sre += err ** 2 / num_points
        cnt = len(image_list)
        mae = float(total_mae / cnt)
        mse = float((total_mse / cnt) ** 0.5)
        nae = float(total_nae / cnt)
        sre = float((total_sre / cnt) ** 0.5)
        return mae, mse, nae, sre

    if threshold is None:
        mae, mse, nae, sre = [], [], [], []
        thresholds = np.arange(0, 1., 0.01)
        for thresh in thresholds:
            mae_, mse_, nae_, sre_ = eval_counting(thresh)
            mae.append(mae_)
            mse.append(mse_)
            nae.append(nae_)
            sre.append(sre_)
        # mae, mse, nae, sre, thresh = mae[np.argmin(mae)], mse[np.argmin(mae)], nae[np.argmin(mae)], sre[np.argmin(mae)], \
        #     thresholds[np.argmin(mae)]
        best_idx = np.argmin(mae)
        mae, mse, nae, sre, thresh = mae[best_idx], mse[best_idx], nae[best_idx], sre[best_idx], thresholds[best_idx]
        results['THRESH'] = thresh
    else:
        mae, mse, nae, sre = eval_counting(threshold)
    results[split + '_MAE'] = mae
    results[split + '_RMSE'] = mse
    results[split + '_NAE'] = nae
    results[split + '_SRE'] = sre
    return results


# ä¸»è®­ç»ƒå¾ªç¯ï¼šè¿­ä»£è®­ç»ƒæ¨¡å‹ï¼Œæ¯1000æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡éªŒè¯å’Œæµ‹è¯•è¯„ä¼°
for n_iter in range(1, max_iter + 1):
    cls_head.train()

    targets = [all_data[all_image_list['train'][i]] for i in torch.randint(0, len(all_image_list['train']), (bs,))]
    targets = convert_to_cuda(targets)
    features = torch.cat([t['features'] for t in targets])
    num_anchors = 256
    pos_ratios = 0.25
    anchor_boxes = []
    query_features, query_labels = [], []
    # è®­ç»ƒé˜¶æ®µ
    for t in targets:

        fname = t['image_id']

        annotations = t['annotations']
        gt_bboxes = annotations['boxes']
        gt_points = annotations['points']

        min_scores = 0.05
        max_points = 1000

        pred_points_score = all_data[fname]['predictions']['pred_points_score']
        mask = torch.zeros(pred_points_score.size(0))
        mask[:min(pred_points_score.size(0), max_points)] = 1
        mask[pred_points_score < min_scores] = 0
        candidate_boxes = all_data[fname]['predictions']['pred_boxes'][mask.bool()].cuda()[:, :num_masks]

        iou_scores = vision_ops.box_iou(candidate_boxes.reshape(-1, 4), gt_bboxes)
        iou_scores = iou_scores.max(dim=1).values.reshape(-1, num_masks)

        anchor_indices = torch.randint(0, candidate_boxes.size(0), (num_anchors,))
        pos_mask = (iou_scores.max(1).values > 0.5).float()
        if pos_mask.sum() > 0:
            pos_indices = torch.multinomial(pos_mask, int(min(num_anchors * pos_ratios, pos_mask.sum())))
            anchor_indices[:len(pos_indices)] = pos_indices

        cur_labels = torch.zeros(len(anchor_indices), num_masks).cuda()
        cur_labels[iou_scores[anchor_indices] > 0.5] = 1.
        query_labels.append(cur_labels)
        anchor_boxes.append(candidate_boxes[anchor_indices])
        if opts.zeroshot:
            example_features = clip_text_prompts[t['class_name']].cuda().unsqueeze(0)
        else:
            example_features = all_data[fname]['example_clip_features'][
                torch.randint(0, len(all_data[fname]['example_clip_features']), (1,))].cuda()
        query_features += [example_features, ] * len(anchor_indices)
    query_labels = torch.cat(query_labels, dim=0)

    targets_a = [all_data[all_image_list['all'][i]] for i in torch.randint(0, len(all_image_list['all']), (bs,))]
    targets_a = convert_to_cuda(targets_a)
    features_a = torch.cat([t['features'] for t in targets_a])
    clip_boxes, clip_target_features, clip_query_labels = [], [], []
    for t in targets_a:
        fname = t['image_id']

        region_boxes = all_data[fname]['predictions']['clip_regions']['boxes'].float().cuda()[:, :num_masks]
        rand_indices = torch.randint(0, len(region_boxes), (min(16, len(region_boxes)),))
        for i in rand_indices:
            iou_scores = vision_ops.box_iou(region_boxes[i], region_boxes[i])
            cur_labels = torch.zeros_like(iou_scores)
            cur_labels[iou_scores > 0.5] = 1.
            clip_query_labels.append(cur_labels)
        clip_boxes.append(region_boxes[rand_indices].cuda())
        clip_target_features += [x[0].cuda() for x in
                                 all_data[fname]['predictions']['clip_regions']['clip_embeddings'][:, :num_masks][
                                     rand_indices].split(1, dim=0)]
    clip_query_labels = torch.cat(clip_query_labels)

    # è®¡ç®—æŸå¤±
    with torch.autograd.set_grad_enabled(True) and torch.autocast(device_type='cuda', enabled=amp):

        cls_outs = cls_head(features, anchor_boxes, query_features)

        cls_loss = F.binary_cross_entropy_with_logits(cls_outs, query_labels, reduction='none')
        loss_mask = (query_labels >= 0).float()
        cls_loss = (cls_loss * loss_mask).sum() / (loss_mask.sum() + 1e-5)

        cls_outs2 = cls_head(features_a, clip_boxes, clip_target_features)
        cls_loss2 = F.binary_cross_entropy_with_logits(cls_outs2, clip_query_labels, reduction='none')
        loss_mask = (clip_query_labels >= 0).float()
        cls_loss2 = (cls_loss2 * loss_mask).sum() / (loss_mask.sum() + 1e-5)

        loss = cls_loss + cls_loss2 * cls_loss2_weight

        update_params = (n_iter % acc_grd_step == 0)
        loss = loss / acc_grd_step
        # åå‘ä¼ æ’­
        scaler(loss, optimizer=optimizer, update_grad=update_params)

    batch_pos_ratio = (query_labels == 1).sum() / ((query_labels == 1).sum() + (query_labels == 0).sum())
    batch_pos_ratio2 = (clip_query_labels == 1).sum() / (
            (clip_query_labels == 1).sum() + (clip_query_labels == 0).sum())
    logger.msg([cls_loss, cls_loss2, batch_pos_ratio, batch_pos_ratio2], n_iter)

    if n_iter % 1000 == 0:
        results = {}
        # set the threshold at val set
        evaluate('val', results)
        evaluate('test', results, results['THRESH'])
        logger.checkpoints(n_iter)
        logger.msg(results, n_iter)

