# PseCoè®­ç»ƒæµç¨‹æ•´åˆæ–¹æ¡ˆå®ç°æŒ‡å—

## ç›®æ ‡

å®ç°ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š
1. **4é˜¶æ®µè®­ç»ƒä¾æ¬¡å¯åœ**ï¼šä¸€é”®è¿è¡Œæ‰€æœ‰è®­ç»ƒé˜¶æ®µï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­
2. **æ”¯æŒæ–°æ•°æ®é›†æ›¿æ¢**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶è½»æ¾åˆ‡æ¢æ•°æ®é›†ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

---

## å®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¶æ„

```
PseCo/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ default_config.yaml          # FSC147é»˜è®¤é…ç½®
â”‚   â””â”€â”€ fscd_lvis_config.yaml        # FSCD-LVISé…ç½®ç¤ºä¾‹
â”œâ”€â”€ train_pipeline.py                # ä¸»è®­ç»ƒå…¥å£ï¼ˆä¸€é”®å¯åŠ¨ï¼‰
â”œâ”€â”€ stages/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ stage1_generate_data.py      # é˜¶æ®µ1ï¼šæ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ stage2_train_heatmap.py      # é˜¶æ®µ2ï¼šè®­ç»ƒç‚¹è§£ç å™¨
â”‚   â”œâ”€â”€ stage3_extract_proposals.py  # é˜¶æ®µ3ï¼šæå–å€™é€‰æ¡†
â”‚   â””â”€â”€ stage4_train_roi_head.py     # é˜¶æ®µ4ï¼šè®­ç»ƒROIåˆ†ç±»å¤´
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config_loader.py              # é…ç½®åŠ è½½å’Œè·¯å¾„è§£æ
    â”œâ”€â”€ stage_checker.py              # æ£€æŸ¥é˜¶æ®µå®ŒæˆçŠ¶æ€
    â””â”€â”€ path_manager.py               # ç»Ÿä¸€è·¯å¾„ç®¡ç†
```

---

## å®ç°æ­¥éª¤

### æ­¥éª¤1ï¼šåˆ›å»ºé…ç½®æ–‡ä»¶ç³»ç»Ÿ

#### 1.1 é…ç½®æ–‡ä»¶æ ¼å¼ (`config/default_config.yaml`)

```yaml
# ==================== é¡¹ç›®é…ç½® ====================
project_root: "/mnt/mydisk/wjj/PseCo-main"

# ==================== æ•°æ®é›†é…ç½® ====================
dataset:
  name: "fsc147"
  root: "/mnt/mydisk/wjj/dataset/FSC_147"
  annotation_file: "{dataset.root}/annotation_FSC147_384_with_gt.json"
  image_dir: "{dataset.root}/images_384_VarV2"
  coco_annotation: "{dataset.root}/instances_test_val_bin.json"

# ==================== æ¨¡å‹è·¯å¾„ ====================
models:
  sam:
    checkpoint: "/mnt/mydisk/wjj/Prompt_sam_localization/checkpoint/sam_vit_h_4b8939.pth"
    arch: "vith"  # vitb, vitl, vith
  clip:
    checkpoint: "{project_root}/ops/foundation_models/clip/ViT-B-32.pt"

# ==================== è¾“å‡ºè·¯å¾„ ====================
outputs:
  data_dir: "{project_root}/data/{dataset.name}/sam"
  checkpoint_dir: "{project_root}/data/{dataset.name}/checkpoints"
  log_dir: "{project_root}/logs"

# ==================== è®­ç»ƒé…ç½® ====================
training:
  stage1:
    enabled: true
    # é˜¶æ®µ1æ˜¯æ¨ç†æµç¨‹ï¼Œæ— è®­ç»ƒå‚æ•°
  stage2:
    enabled: true
    max_iter: 50000
    batch_size: 64
    lr: 0.0001
    weight_decay: 0.0
    save_interval: 1000
  stage3:
    enabled: true
    num_gpus: 4
    master_port: 17673
  stage4:
    enabled: true
    max_iter: 10000
    batch_size: 32
    lr: 0.0001
    weight_decay: 0.00001
    eval_interval: 1000
    mode: "fewshot"  # fewshot, zeroshot, vild
    run_name: "MLP_small_box_w1"
    wandb:
      enabled: false
      entity: "zzhuang"
      project: "Counting"

# ==================== GPUé…ç½® ====================
gpu:
  device: 0  # å•GPUè®­ç»ƒæ—¶çš„è®¾å¤‡ID
  distributed: false  # æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆé˜¶æ®µ3å¿…é¡»ä¸ºtrueï¼‰
```

#### 1.2 é…ç½®åŠ è½½å™¨ (`utils/config_loader.py`)

```python
"""
é…ç½®åŠ è½½å™¨ï¼šåŠ è½½YAMLé…ç½®å¹¶è§£æè·¯å¾„æ¨¡æ¿
"""
import yaml
import os
from pathlib import Path
from typing import Dict, Any

class ConfigLoader:
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = self._load_yaml(config_path)
        self.config = self._resolve_paths(self.config)
    
    def _load_yaml(self, path: str) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _resolve_paths(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè·¯å¾„æ¨¡æ¿å˜é‡"""
        # å…ˆè§£æproject_rootå’Œdataset.root
        project_root = config.get('project_root', '')
        dataset_root = config.get('dataset', {}).get('root', '')
        dataset_name = config.get('dataset', {}).get('name', '')
        
        # é€’å½’æ›¿æ¢æ‰€æœ‰è·¯å¾„æ¨¡æ¿
        def replace_vars(obj, context=None):
            if context is None:
                context = {
                    'project_root': project_root,
                    'dataset.root': dataset_root,
                    'dataset.name': dataset_name,
                }
            
            if isinstance(obj, dict):
                return {k: replace_vars(v, context) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_vars(item, context) for item in obj]
            elif isinstance(obj, str):
                # æ›¿æ¢æ¨¡æ¿å˜é‡
                for key, value in context.items():
                    obj = obj.replace(f'{{{key}}}', str(value))
                return obj
            else:
                return obj
        
        return replace_vars(config)
    
    def get(self, key: str, default=None):
        """è·å–é…ç½®å€¼ï¼ˆæ”¯æŒç‚¹å·åˆ†éš”çš„åµŒå¥—é”®ï¼‰"""
        keys = key.split('.')
        value = self.config
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
                if value is None:
                    return default
            else:
                return default
        return value
    
    def __getitem__(self, key: str):
        """æ”¯æŒå­—å…¸å¼è®¿é—®"""
        return self.get(key)

def load_config(config_path: str = 'config/default_config.yaml') -> ConfigLoader:
    """åŠ è½½é…ç½®æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    return ConfigLoader(config_path)
```

#### 1.3 è·¯å¾„ç®¡ç†å™¨ (`utils/path_manager.py`)

```python
"""
ç»Ÿä¸€è·¯å¾„ç®¡ç†å™¨ï¼šåŸºäºé…ç½®ç”Ÿæˆæ‰€æœ‰è·¯å¾„
"""
from utils.config_loader import ConfigLoader
from pathlib import Path
from typing import Optional

class PathManager:
    def __init__(self, config: ConfigLoader):
        self.config = config
        self.project_root = Path(config.get('project_root'))
        self.dataset_root = Path(config.get('dataset.root'))
        self.dataset_name = config.get('dataset.name')
    
    # ========== æ•°æ®é›†è·¯å¾„ ==========
    def get_annotation_file(self) -> Path:
        return Path(self.config.get('dataset.annotation_file'))
    
    def get_image_dir(self) -> Path:
        return Path(self.config.get('dataset.image_dir'))
    
    def get_coco_annotation(self) -> Path:
        return Path(self.config.get('dataset.coco_annotation'))
    
    # ========== æ¨¡å‹è·¯å¾„ ==========
    def get_sam_checkpoint(self) -> Path:
        return Path(self.config.get('models.sam.checkpoint'))
    
    def get_clip_checkpoint(self) -> Path:
        return Path(self.config.get('models.clip.checkpoint'))
    
    # ========== è¾“å‡ºè·¯å¾„ ==========
    def get_data_dir(self) -> Path:
        data_dir = Path(self.config.get('outputs.data_dir'))
        data_dir.mkdir(parents=True, exist_ok=True)
        return data_dir
    
    def get_checkpoint_dir(self) -> Path:
        checkpoint_dir = Path(self.config.get('outputs.checkpoint_dir'))
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        return checkpoint_dir
    
    def get_log_dir(self) -> Path:
        log_dir = Path(self.config.get('outputs.log_dir'))
        log_dir.mkdir(parents=True, exist_ok=True)
        return log_dir
    
    # ========== é˜¶æ®µè¾“å‡ºæ–‡ä»¶ ==========
    def get_stage1_output_all_data(self, version: str = 'vith_v5_fix') -> Path:
        """é˜¶æ®µ1è¾“å‡ºï¼šall_dataæ–‡ä»¶"""
        return self.get_data_dir() / f'all_data_{version}.pth'
    
    def get_stage1_output_segment_data(self) -> Path:
        """é˜¶æ®µ1è¾“å‡ºï¼šsegment_anything_data"""
        return self.get_data_dir() / 'segment_anything_data_vith.pth'
    
    def get_stage1_output_pseudo_boxes(self) -> Path:
        """é˜¶æ®µ1è¾“å‡ºï¼špseudo_boxes"""
        return self.get_data_dir() / 'pseudo_boxes_data_vith.pth'
    
    def get_stage1_output_clip_text(self) -> Path:
        """é˜¶æ®µ1è¾“å‡ºï¼šCLIPæ–‡æœ¬æç¤º"""
        return self.get_data_dir().parent / 'clip_text_prompt.pth'
    
    def get_stage2_output_point_decoder(self, version: str = 'vith_v5') -> Path:
        """é˜¶æ®µ2è¾“å‡ºï¼špoint_decoderæƒé‡"""
        checkpoint_dir = self.get_checkpoint_dir()
        return checkpoint_dir / f'point_decoder_{version}.pth'
    
    def get_stage3_output_predictions(self) -> Path:
        """é˜¶æ®µ3è¾“å‡ºï¼šall_predictions"""
        return self.get_data_dir() / 'all_predictions_vith.pth'
    
    def get_stage4_output_dir(self, run_name: str) -> Path:
        """é˜¶æ®µ4è¾“å‡ºï¼šROIåˆ†ç±»å¤´æƒé‡ç›®å½•"""
        return self.get_checkpoint_dir() / 'cls_head' / 'ckpt' / run_name
```

---

### æ­¥éª¤2ï¼šåˆ›å»ºé˜¶æ®µæ£€æŸ¥å™¨

#### 2.1 é˜¶æ®µå®Œæˆæ£€æŸ¥å™¨ (`utils/stage_checker.py`)

```python
"""
æ£€æŸ¥å„è®­ç»ƒé˜¶æ®µçš„å®ŒæˆçŠ¶æ€
"""
import torch
from pathlib import Path
from utils.path_manager import PathManager
from utils.config_loader import ConfigLoader

class StageChecker:
    def __init__(self, config: ConfigLoader):
        self.config = config
        self.paths = PathManager(config)
    
    def check_stage1(self) -> bool:
        """æ£€æŸ¥é˜¶æ®µ1æ˜¯å¦å®Œæˆ"""
        required_files = [
            self.paths.get_stage1_output_all_data(),
            self.paths.get_stage1_output_segment_data(),
            self.paths.get_stage1_output_pseudo_boxes(),
            self.paths.get_stage1_output_clip_text(),
        ]
        return all(f.exists() and f.stat().st_size > 0 for f in required_files)
    
    def check_stage2(self) -> bool:
        """æ£€æŸ¥é˜¶æ®µ2æ˜¯å¦å®Œæˆ"""
        checkpoint = self.paths.get_stage2_output_point_decoder()
        if not checkpoint.exists():
            return False
        # æ£€æŸ¥checkpointæ˜¯å¦åŒ…å«æ¨¡å‹æƒé‡
        try:
            state_dict = torch.load(checkpoint, map_location='cpu')
            if isinstance(state_dict, dict):
                # å¯èƒ½æ˜¯ {'point_decoder': ..., 'optimizer': ...} æ ¼å¼
                return 'point_decoder' in state_dict or any('transformer' in k for k in state_dict.keys())
            return True
        except:
            return False
    
    def check_stage3(self) -> bool:
        """æ£€æŸ¥é˜¶æ®µ3æ˜¯å¦å®Œæˆ"""
        predictions_file = self.paths.get_stage3_output_predictions()
        if not predictions_file.exists():
            return False
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        try:
            data = torch.load(predictions_file, map_location='cpu')
            return isinstance(data, dict) and len(data) > 0
        except:
            return False
    
    def check_stage4(self) -> bool:
        """æ£€æŸ¥é˜¶æ®µ4æ˜¯å¦å®Œæˆï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆcheckpointï¼‰"""
        run_name = self.config.get('training.stage4.run_name')
        if self.config.get('training.stage4.mode') == 'zeroshot':
            run_name += '_zeroshot'
        output_dir = self.paths.get_stage4_output_dir(run_name)
        # æ£€æŸ¥æ˜¯å¦æœ‰checkpointæ–‡ä»¶
        checkpoint_files = list(output_dir.glob('*.pth')) if output_dir.exists() else []
        return len(checkpoint_files) > 0
    
    def check_stage(self, stage_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šé˜¶æ®µæ˜¯å¦å®Œæˆ"""
        checkers = {
            '1': self.check_stage1,
            '2': self.check_stage2,
            '3': self.check_stage3,
            '4': self.check_stage4,
        }
        if stage_id not in checkers:
            raise ValueError(f"æœªçŸ¥é˜¶æ®µ: {stage_id}")
        return checkers[stage_id]()
    
    def get_stage_status(self) -> dict:
        """è·å–æ‰€æœ‰é˜¶æ®µçš„å®ŒæˆçŠ¶æ€"""
        return {
            '1': self.check_stage1(),
            '2': self.check_stage2(),
            '3': self.check_stage3(),
            '4': self.check_stage4(),
        }
```

---

### æ­¥éª¤3ï¼šé‡æ„å„é˜¶æ®µä¸ºå‡½æ•°æ¨¡å—

#### 3.1 é˜¶æ®µ1é‡æ„ç¤ºä¾‹ (`stages/stage1_generate_data.py`)

```python
"""
é˜¶æ®µ1ï¼šæ•°æ®é¢„å¤„ç†
å°†åŸæœ‰çš„1_generate_data.pyé‡æ„ä¸ºå‡½æ•°å½¢å¼
"""
import sys
import os
import torch
from pathlib import Path
from utils.config_loader import ConfigLoader
from utils.path_manager import PathManager

def run_stage1(config: ConfigLoader):
    """
    æ‰§è¡Œé˜¶æ®µ1ï¼šæ•°æ®é¢„å¤„ç†
    
    Args:
        config: é…ç½®åŠ è½½å™¨å¯¹è±¡
    """
    from ops.foundation_models.segment_anything import build_sam_vit_h
    from ops.foundation_models import clip
    import json
    import tqdm
    from PIL import Image
    import numpy as np
    import albumentations as A
    from torchvision.transforms.functional import to_tensor
    import torch.nn.functional as F
    
    # åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
    paths = PathManager(config)
    
    # è®¾ç½®GPU
    gpu_id = config.get('gpu.device', 0)
    torch.cuda.set_device(gpu_id)
    torch.autograd.set_grad_enabled(False)
    
    print("=" * 60)
    print("é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†")
    print("=" * 60)
    
    # 1. åŠ è½½SAMæ¨¡å‹
    print("åŠ è½½SAMæ¨¡å‹...")
    sam_arch = config.get('models.sam.arch', 'vith')
    sam_checkpoint = paths.get_sam_checkpoint()
    
    if sam_arch == 'vith':
        sam = build_sam_vit_h(str(sam_checkpoint)).cuda().eval()
    elif sam_arch == 'vitl':
        sam = build_sam_vit_l(str(sam_checkpoint)).cuda().eval()
    elif sam_arch == 'vitb':
        sam = build_sam_vit_b(str(sam_checkpoint)).cuda().eval()
    else:
        raise ValueError(f"æœªçŸ¥SAMæ¶æ„: {sam_arch}")
    
    # 2. åŠ è½½CLIPæ¨¡å‹
    print("åŠ è½½CLIPæ¨¡å‹...")
    clip_checkpoint = paths.get_clip_checkpoint()
    model, preprocess = clip.load(str(clip_checkpoint))
    model.cuda().eval()
    
    # 3. åŠ è½½æ ‡æ³¨æ–‡ä»¶
    print("åŠ è½½æ ‡æ³¨æ–‡ä»¶...")
    annotation_file = paths.get_annotation_file()
    with open(annotation_file, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
    
    # 4. å›¾åƒé¢„å¤„ç†å‡½æ•°
    image_dir = paths.get_image_dir()
    def read_image(fname):
        img_path = image_dir / fname
        img = Image.open(str(img_path))
        transform = A.Compose([
            A.LongestMaxSize(1024),
            A.PadIfNeeded(1024, border_mode=0, position=A.PadIfNeeded.PositionType.TOP_LEFT),
        ])
        img = Image.fromarray(transform(image=np.array(img))['image'])
        return img
    
    # 5. æå–SAMç‰¹å¾ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦å®Œæ•´å®ç°ï¼‰
    print("æå–SAMå›¾åƒç‰¹å¾...")
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    for fname in tqdm.tqdm(all_data):
        image = read_image(fname)
        with torch.no_grad():
            new_image = transform(image).unsqueeze(0).cuda()
            features = sam.image_encoder(new_image)
        all_data[fname]['features'] = features.cpu()
    
    # 6. ä¿å­˜all_data
    output_file = paths.get_stage1_output_all_data()
    output_file.parent.mkdir(parents=True, exist_ok=True)
    torch.save(all_data, str(output_file), pickle_protocol=5)
    print(f"âœ… ä¿å­˜all_dataåˆ°: {output_file}")
    
    # 7. å…¶ä»–å¤„ç†ï¼ˆsegment_anything_data, pseudo_boxes, clip_text_promptï¼‰
    # ... (è¿™é‡Œéœ€è¦å®Œæ•´å®ç°åŸæœ‰é€»è¾‘)
    
    print("âœ… é˜¶æ®µ1å®Œæˆï¼")

if __name__ == '__main__':
    # æµ‹è¯•ç”¨
    from utils.config_loader import load_config
    config = load_config('config/default_config.yaml')
    run_stage1(config)
```

#### 3.2 å…¶ä»–é˜¶æ®µç±»ä¼¼é‡æ„

é˜¶æ®µ2ã€3ã€4ä¹Ÿéœ€è¦ç±»ä¼¼é‡æ„ï¼Œå°†åŸæœ‰è„šæœ¬çš„ä¸»è¦é€»è¾‘æå–ä¸º`run_stage2(config)`, `run_stage3(config)`, `run_stage4(config)`å‡½æ•°ã€‚

---

### æ­¥éª¤4ï¼šåˆ›å»ºä¸»è®­ç»ƒå…¥å£

#### 4.1 ä¸»è®­ç»ƒè„šæœ¬ (`train_pipeline.py`)

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PseCoç»Ÿä¸€è®­ç»ƒå…¥å£
æ”¯æŒä¸€é”®è¿è¡Œ4ä¸ªè®­ç»ƒé˜¶æ®µï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­å’Œæ•°æ®é›†é…ç½®
"""
import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.config_loader import load_config
from utils.stage_checker import StageChecker
from utils.path_manager import PathManager

# å¯¼å…¥å„é˜¶æ®µå‡½æ•°
from stages.stage1_generate_data import run_stage1
from stages.stage2_train_heatmap import run_stage2
from stages.stage3_extract_proposals import run_stage3
from stages.stage4_train_roi_head import run_stage4


def print_stage_status(checker: StageChecker):
    """æ‰“å°å„é˜¶æ®µå®ŒæˆçŠ¶æ€"""
    status = checker.get_stage_status()
    stage_names = {
        '1': 'æ•°æ®é¢„å¤„ç†',
        '2': 'è®­ç»ƒç‚¹è§£ç å™¨',
        '3': 'æå–å€™é€‰æ¡†',
        '4': 'è®­ç»ƒROIåˆ†ç±»å¤´',
    }
    print("\n" + "=" * 60)
    print("è®­ç»ƒé˜¶æ®µå®ŒæˆçŠ¶æ€:")
    print("=" * 60)
    for stage_id, name in stage_names.items():
        completed = "âœ…" if status[stage_id] else "âŒ"
        print(f"  é˜¶æ®µ{stage_id} ({name}): {completed}")
    print("=" * 60 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description='PseCoç»Ÿä¸€è®­ç»ƒå…¥å£ï¼šä¸€é”®è¿è¡Œ4ä¸ªè®­ç»ƒé˜¶æ®µ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä¸€é”®è¿è¡Œæ‰€æœ‰é˜¶æ®µ
  python train_pipeline.py --config config/default_config.yaml
  
  # åªè¿è¡Œç‰¹å®šé˜¶æ®µ
  python train_pipeline.py --stages 3 4
  
  # æ–­ç‚¹ç»­è®­ï¼ˆè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µï¼‰
  python train_pipeline.py --resume
  
  # ä½¿ç”¨æ–°æ•°æ®é›†é…ç½®
  python train_pipeline.py --config config/fscd_lvis_config.yaml
        """
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/default_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config/default_config.yamlï¼‰'
    )
    parser.add_argument(
        '--stages',
        type=str,
        nargs='+',
        default=['1', '2', '3', '4'],
        help='è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨ï¼ˆé»˜è®¤: 1 2 3 4ï¼‰'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='æ–­ç‚¹ç»­è®­ï¼šè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µ'
    )
    parser.add_argument(
        '--check-only',
        action='store_true',
        help='ä»…æ£€æŸ¥é˜¶æ®µå®ŒæˆçŠ¶æ€ï¼Œä¸æ‰§è¡Œè®­ç»ƒ'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°æ‰§è¡Œæ‰€æœ‰é˜¶æ®µï¼ˆå³ä½¿å·²å®Œæˆï¼‰'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    try:
        config = load_config(args.config)
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   é¡¹ç›®æ ¹ç›®å½•: {config.get('project_root')}")
        print(f"   æ•°æ®é›†: {config.get('dataset.name')}")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åˆå§‹åŒ–æ£€æŸ¥å™¨
    checker = StageChecker(config)
    
    # æ‰“å°é˜¶æ®µçŠ¶æ€
    print_stage_status(checker)
    
    # å¦‚æœåªæ˜¯æ£€æŸ¥ï¼Œåˆ™é€€å‡º
    if args.check_only:
        return 0
    
    # å®šä¹‰é˜¶æ®µæ˜ å°„
    stage_funcs = {
        '1': (run_stage1, 'æ•°æ®é¢„å¤„ç†'),
        '2': (run_stage2, 'è®­ç»ƒç‚¹è§£ç å™¨'),
        '3': (run_stage3, 'æå–å€™é€‰æ¡†'),
        '4': (run_stage4, 'è®­ç»ƒROIåˆ†ç±»å¤´'),
    }
    
    # æ‰§è¡Œé˜¶æ®µ
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    for stage_id in args.stages:
        if stage_id not in stage_funcs:
            print(f"âŒ æœªçŸ¥é˜¶æ®µ: {stage_id}")
            fail_count += 1
            continue
        
        func, name = stage_funcs[stage_id]
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        is_completed = checker.check_stage(stage_id)
        if args.resume and is_completed and not args.force:
            print(f"â­ï¸  é˜¶æ®µ{stage_id} ({name}) å·²å®Œæˆï¼Œè·³è¿‡")
            skip_count += 1
            continue
        
        if args.force and is_completed:
            print(f"âš ï¸  é˜¶æ®µ{stage_id} ({name}) å·²å®Œæˆï¼Œä½†ä½¿ç”¨--forceå¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
        
        # æ‰§è¡Œé˜¶æ®µ
        print("\n" + "=" * 60)
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œé˜¶æ®µ{stage_id}: {name}")
        print("=" * 60)
        
        try:
            func(config)
            print(f"âœ… é˜¶æ®µ{stage_id} ({name}) å®Œæˆ")
            success_count += 1
        except KeyboardInterrupt:
            print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œé˜¶æ®µ{stage_id} ({name}) æœªå®Œæˆ")
            print("ğŸ’¡ æç¤º: ä½¿ç”¨ --resume å¯ä»¥æ–­ç‚¹ç»­è®­")
            return 1
        except Exception as e:
            print(f"âŒ é˜¶æ®µ{stage_id} ({name}) å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            fail_count += 1
            if not args.resume:
                print("âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡º")
                return 1
            else:
                print("âš ï¸  ç»§ç»­æ‰§è¡Œä¸‹ä¸€é˜¶æ®µ...")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("è®­ç»ƒæ€»ç»“:")
    print("=" * 60)
    print(f"  âœ… æˆåŠŸ: {success_count} ä¸ªé˜¶æ®µ")
    print(f"  â­ï¸  è·³è¿‡: {skip_count} ä¸ªé˜¶æ®µ")
    print(f"  âŒ å¤±è´¥: {fail_count} ä¸ªé˜¶æ®µ")
    print("=" * 60)
    
    return 0 if fail_count == 0 else 1


if __name__ == '__main__':
    sys.exit(main())
```

---

## ä½¿ç”¨æ–¹å¼

### 1. åŸºæœ¬ä½¿ç”¨

```bash
# ä¸€é”®è¿è¡Œæ‰€æœ‰é˜¶æ®µ
python train_pipeline.py --config config/default_config.yaml

# åªè¿è¡Œç‰¹å®šé˜¶æ®µ
python train_pipeline.py --stages 3 4

# æ–­ç‚¹ç»­è®­
python train_pipeline.py --resume

# æ£€æŸ¥é˜¶æ®µå®ŒæˆçŠ¶æ€
python train_pipeline.py --check-only
```

### 2. ä½¿ç”¨æ–°æ•°æ®é›†

åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶ `config/my_dataset_config.yaml`ï¼š

```yaml
project_root: "/path/to/PseCo"
dataset:
  name: "my_dataset"
  root: "/path/to/my_dataset"
  annotation_file: "{dataset.root}/annotations.json"
  image_dir: "{dataset.root}/images"
  # ... å…¶ä»–é…ç½®
```

ç„¶åè¿è¡Œï¼š
```bash
python train_pipeline.py --config config/my_dataset_config.yaml
```

---

## å®æ–½ä¼˜å…ˆçº§

### é˜¶æ®µ1ï¼šæœ€å°å¯è¡Œæ–¹æ¡ˆï¼ˆMVPï¼‰
1. âœ… åˆ›å»ºé…ç½®æ–‡ä»¶ç³»ç»Ÿ
2. âœ… åˆ›å»ºä¸»è®­ç»ƒå…¥å£è„šæœ¬
3. âœ… å®ç°é˜¶æ®µæ£€æŸ¥å™¨
4. âš ï¸ ç®€å•åŒ…è£…ç°æœ‰è„šæœ¬ï¼ˆä¸é‡æ„ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’é…ç½®ï¼‰

### é˜¶æ®µ2ï¼šå®Œæ•´é‡æ„
1. é‡æ„å„é˜¶æ®µä¸ºå‡½æ•°æ¨¡å—
2. ç»Ÿä¸€è·¯å¾„ç®¡ç†
3. å®Œå–„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

### é˜¶æ®µ3ï¼šå¢å¼ºåŠŸèƒ½
1. æ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼ˆå¦‚æœé˜¶æ®µé—´æ— ä¾èµ–ï¼‰
2. æ”¯æŒè‡ªå®šä¹‰é˜¶æ®µé¡ºåº
3. æ·»åŠ è¿›åº¦æ¡å’ŒETAä¼°ç®—
4. æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒé…ç½®

---

## æ³¨æ„äº‹é¡¹

1. **å‘åå…¼å®¹**ï¼šä¿æŒåŸæœ‰è„šæœ¬å¯ç”¨ï¼Œæ–°ç³»ç»Ÿä½œä¸ºå¯é€‰æ–¹æ¡ˆ
2. **æµ‹è¯•éªŒè¯**ï¼šæ¯ä¸ªé˜¶æ®µé‡æ„åéƒ½è¦æµ‹è¯•
3. **æ–‡æ¡£æ›´æ–°**ï¼šæ›´æ–°READMEï¼Œè¯´æ˜æ–°ä½¿ç”¨æ–¹æ³•
4. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å¤±è´¥æ—¶èƒ½æ¢å¤
